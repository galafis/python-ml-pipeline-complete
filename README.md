# üáßüá∑ Pipeline Completo de Machine Learning | üá∫üá∏ Complete Machine Learning Pipeline

<div align="center">

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Scikit-Learn](https://img.shields.io/badge/scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)
![MLflow](https://img.shields.io/badge/MLflow-0194E2?style=for-the-badge&logo=mlflow&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)

**Pipeline end-to-end de Machine Learning com MLOps e deployment em produ√ß√£o**

[üöÄ Features](#-funcionalidades) ‚Ä¢ [üìä Modelos](#-modelos-implementados) ‚Ä¢ [‚ö° Quick Start](#-quick-start) ‚Ä¢ [üîß MLOps](#-mlops)

</div>

---

## üáßüá∑ Portugu√™s

### üöÄ Vis√£o Geral

Pipeline **completo de Machine Learning** desenvolvido em Python, implementando as melhores pr√°ticas de MLOps:

- üîÑ **Pipeline End-to-End**: Desde ingest√£o de dados at√© deployment
- üß† **M√∫ltiplos Algoritmos**: Classifica√ß√£o, regress√£o, clustering
- üìä **Monitoramento**: Tracking de experimentos com MLflow
- üê≥ **Containeriza√ß√£o**: Docker para deployment consistente
- üåê **API REST**: FastAPI para servir modelos em produ√ß√£o
- üìà **Visualiza√ß√µes**: Dashboards interativos com Streamlit

### üéØ Objetivos do Pipeline

- **Automatizar** todo o ciclo de vida de ML
- **Padronizar** processos de desenvolvimento
- **Facilitar** deployment e monitoramento
- **Garantir** reprodutibilidade de experimentos
- **Acelerar** time-to-market de modelos

### üõ†Ô∏è Stack Tecnol√≥gico

#### Machine Learning Core
- **scikit-learn**: Algoritmos de ML cl√°ssicos
- **xgboost**: Gradient boosting avan√ßado
- **lightgbm**: Gradient boosting eficiente
- **catboost**: Gradient boosting para dados categ√≥ricos
- **optuna**: Otimiza√ß√£o de hiperpar√¢metros

#### Deep Learning
- **tensorflow**: Framework de deep learning
- **keras**: API de alto n√≠vel para TensorFlow
- **pytorch**: Framework alternativo de DL
- **transformers**: Modelos de linguagem pr√©-treinados

#### Data Processing
- **pandas**: Manipula√ß√£o de dados estruturados
- **numpy**: Computa√ß√£o num√©rica
- **polars**: Processamento r√°pido de dados
- **dask**: Computa√ß√£o paralela e distribu√≠da

#### MLOps e Deployment
- **mlflow**: Tracking e gerenciamento de experimentos
- **dvc**: Versionamento de dados e modelos
- **docker**: Containeriza√ß√£o
- **kubernetes**: Orquestra√ß√£o de containers
- **fastapi**: API REST para servir modelos

#### Visualiza√ß√£o e Monitoramento
- **streamlit**: Dashboards interativos
- **plotly**: Gr√°ficos interativos
- **wandb**: Monitoramento de experimentos
- **evidently**: Monitoramento de drift de dados

### üìã Estrutura do Pipeline

```
python-ml-pipeline-complete/
‚îú‚îÄ‚îÄ üìÅ src/                        # C√≥digo fonte principal
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ data/                   # M√≥dulos de dados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ ingestion.py        # Ingest√£o de dados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ validation.py       # Valida√ß√£o de dados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ preprocessing.py    # Pr√©-processamento
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ feature_engineering.py # Engenharia de features
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ models/                 # M√≥dulos de modelos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ base_model.py       # Classe base para modelos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ classification.py   # Modelos de classifica√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ regression.py       # Modelos de regress√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ clustering.py       # Modelos de clustering
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ ensemble.py         # Modelos ensemble
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ training/               # M√≥dulos de treinamento
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ trainer.py          # Classe principal de treinamento
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ hyperparameter_tuning.py # Otimiza√ß√£o de hiperpar√¢metros
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ cross_validation.py # Valida√ß√£o cruzada
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ model_selection.py  # Sele√ß√£o de modelos
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ evaluation/             # M√≥dulos de avalia√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ metrics.py          # M√©tricas de avalia√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ visualization.py    # Visualiza√ß√µes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ reports.py          # Relat√≥rios autom√°ticos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ model_interpretation.py # Interpretabilidade
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ deployment/             # M√≥dulos de deployment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ api.py              # API FastAPI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ batch_prediction.py # Predi√ß√µes em lote
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ model_serving.py    # Servir modelos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ monitoring.py       # Monitoramento em produ√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ utils/                  # Utilit√°rios
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ config.py           # Configura√ß√µes
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ logging.py          # Sistema de logs
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ database.py         # Conex√µes de banco
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ helpers.py          # Fun√ß√µes auxiliares
‚îú‚îÄ‚îÄ üìÅ data/                       # Dados do projeto
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ raw/                    # Dados brutos
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ processed/              # Dados processados
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ features/               # Features engineered
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ external/               # Dados externos
‚îú‚îÄ‚îÄ üìÅ models/                     # Modelos treinados
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ experiments/            # Experimentos MLflow
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ production/             # Modelos em produ√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ artifacts/              # Artefatos de modelo
‚îú‚îÄ‚îÄ üìÅ notebooks/                  # Jupyter notebooks
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 01_data_exploration.ipynb # Explora√ß√£o de dados
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 02_feature_engineering.ipynb # Engenharia de features
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 03_model_development.ipynb # Desenvolvimento de modelos
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 04_model_evaluation.ipynb # Avalia√ß√£o de modelos
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ 05_model_interpretation.ipynb # Interpreta√ß√£o de modelos
‚îú‚îÄ‚îÄ üìÅ tests/                      # Testes automatizados
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_data_processing.py # Testes processamento
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_models.py          # Testes modelos
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_api.py             # Testes API
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ test_integration.py     # Testes integra√ß√£o
‚îú‚îÄ‚îÄ üìÅ configs/                    # Arquivos de configura√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ model_config.yaml       # Configura√ß√£o de modelos
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ data_config.yaml        # Configura√ß√£o de dados
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ deployment_config.yaml  # Configura√ß√£o deployment
‚îú‚îÄ‚îÄ üìÅ docker/                     # Arquivos Docker
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile.training     # Container para treinamento
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile.api          # Container para API
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ docker-compose.yml      # Orquestra√ß√£o local
‚îú‚îÄ‚îÄ üìÅ kubernetes/                 # Manifests Kubernetes
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ deployment.yaml         # Deployment
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ service.yaml            # Service
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ ingress.yaml            # Ingress
‚îú‚îÄ‚îÄ üìÅ scripts/                    # Scripts de automa√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ train_model.py          # Script de treinamento
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ evaluate_model.py       # Script de avalia√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ deploy_model.py         # Script de deployment
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ batch_predict.py        # Script predi√ß√£o em lote
‚îú‚îÄ‚îÄ üìÅ streamlit_app/              # Dashboard Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ app.py                  # Aplica√ß√£o principal
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ pages/                  # P√°ginas do dashboard
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ components/             # Componentes reutiliz√°veis
‚îú‚îÄ‚îÄ üìÑ requirements.txt            # Depend√™ncias Python
‚îú‚îÄ‚îÄ üìÑ requirements-dev.txt        # Depend√™ncias desenvolvimento
‚îú‚îÄ‚îÄ üìÑ setup.py                    # Setup do pacote
‚îú‚îÄ‚îÄ üìÑ pyproject.toml             # Configura√ß√£o do projeto
‚îú‚îÄ‚îÄ üìÑ Makefile                   # Comandos automatizados
‚îú‚îÄ‚îÄ üìÑ .github/workflows/         # CI/CD GitHub Actions
‚îú‚îÄ‚îÄ üìÑ README.md                  # Este arquivo
‚îú‚îÄ‚îÄ üìÑ LICENSE                    # Licen√ßa MIT
‚îî‚îÄ‚îÄ üìÑ .gitignore                # Arquivos ignorados
```

### üöÄ Funcionalidades Principais

#### 1. üìä Ingest√£o e Processamento de Dados

**Ingest√£o Flex√≠vel**
```python
from src.data.ingestion import DataIngestion

# M√∫ltiplas fontes de dados
ingestion = DataIngestion()

# Banco de dados
data_db = ingestion.from_database(
    connection_string="postgresql://user:pass@host:port/db",
    query="SELECT * FROM sales_data WHERE date >= '2024-01-01'"
)

# APIs REST
data_api = ingestion.from_api(
    url="https://api.example.com/data",
    headers={"Authorization": "Bearer token"},
    params={"limit": 10000}
)

# Arquivos (CSV, Parquet, JSON)
data_file = ingestion.from_file(
    file_path="data/raw/sales_data.csv",
    file_type="csv",
    parse_dates=["date"]
)
```

**Valida√ß√£o de Dados**
```python
from src.data.validation import DataValidator

validator = DataValidator()

# Valida√ß√£o de schema
schema_validation = validator.validate_schema(
    data=df,
    expected_columns=["id", "feature1", "feature2", "target"],
    column_types={"id": "int64", "feature1": "float64"}
)

# Detec√ß√£o de anomalias
anomalies = validator.detect_anomalies(
    data=df,
    methods=["isolation_forest", "local_outlier_factor"],
    contamination=0.1
)

# Verifica√ß√£o de qualidade
quality_report = validator.data_quality_report(df)
```

#### 2. üîß Engenharia de Features

**Feature Engineering Automatizada**
```python
from src.data.feature_engineering import FeatureEngineer

fe = FeatureEngineer()

# Features temporais
temporal_features = fe.create_temporal_features(
    data=df,
    date_column="date",
    features=["year", "month", "day_of_week", "is_weekend"]
)

# Features de agrega√ß√£o
agg_features = fe.create_aggregation_features(
    data=df,
    group_by=["customer_id"],
    agg_columns=["amount"],
    agg_functions=["mean", "sum", "std", "count"]
)

# Features de intera√ß√£o
interaction_features = fe.create_interaction_features(
    data=df,
    feature_pairs=[("feature1", "feature2"), ("feature3", "feature4")]
)

# Sele√ß√£o autom√°tica de features
selected_features = fe.select_features(
    X=X_train,
    y=y_train,
    method="mutual_info",
    k_best=20
)
```

#### 3. üß† Modelos de Machine Learning

**Classifica√ß√£o**
```python
from src.models.classification import ClassificationPipeline

# Pipeline de classifica√ß√£o
clf_pipeline = ClassificationPipeline()

# M√∫ltiplos algoritmos
models = {
    "random_forest": {"n_estimators": 100, "max_depth": 10},
    "xgboost": {"n_estimators": 100, "learning_rate": 0.1},
    "lightgbm": {"n_estimators": 100, "num_leaves": 31},
    "logistic_regression": {"C": 1.0, "max_iter": 1000}
}

# Treinamento e avalia√ß√£o
results = clf_pipeline.train_and_evaluate(
    X_train=X_train,
    y_train=y_train,
    X_test=X_test,
    y_test=y_test,
    models=models,
    cv_folds=5
)

# Modelo ensemble
ensemble_model = clf_pipeline.create_ensemble(
    models=results["trained_models"],
    method="voting",  # ou "stacking"
    weights=[0.3, 0.3, 0.2, 0.2]
)
```

**Regress√£o**
```python
from src.models.regression import RegressionPipeline

reg_pipeline = RegressionPipeline()

# Modelos de regress√£o
reg_models = {
    "linear_regression": {},
    "random_forest": {"n_estimators": 100},
    "xgboost": {"n_estimators": 100, "learning_rate": 0.1},
    "neural_network": {"hidden_layer_sizes": (100, 50)}
}

# Treinamento com valida√ß√£o cruzada
reg_results = reg_pipeline.train_with_cv(
    X=X_train,
    y=y_train,
    models=reg_models,
    cv_folds=5,
    scoring=["r2", "mse", "mae"]
)
```

#### 4. üéØ Otimiza√ß√£o de Hiperpar√¢metros

**Optuna para Otimiza√ß√£o Bayesiana**
```python
from src.training.hyperparameter_tuning import HyperparameterTuner

tuner = HyperparameterTuner()

# Definir espa√ßo de busca
search_space = {
    "n_estimators": ("int", 50, 500),
    "max_depth": ("int", 3, 20),
    "learning_rate": ("float", 0.01, 0.3),
    "subsample": ("float", 0.6, 1.0)
}

# Otimiza√ß√£o
best_params = tuner.optimize(
    model_class="xgboost",
    X_train=X_train,
    y_train=y_train,
    search_space=search_space,
    n_trials=100,
    cv_folds=5,
    scoring="roc_auc"
)

# Treinamento com melhores par√¢metros
best_model = tuner.train_best_model(
    best_params=best_params,
    X_train=X_train,
    y_train=y_train
)
```

#### 5. üìà Avalia√ß√£o e Interpretabilidade

**M√©tricas Abrangentes**
```python
from src.evaluation.metrics import ModelEvaluator

evaluator = ModelEvaluator()

# Avalia√ß√£o completa
evaluation_report = evaluator.comprehensive_evaluation(
    model=trained_model,
    X_test=X_test,
    y_test=y_test,
    task_type="classification"
)

# M√©tricas incluem:
# - Accuracy, Precision, Recall, F1-score
# - ROC-AUC, PR-AUC
# - Confusion Matrix
# - Classification Report
# - Feature Importance
```

**Interpretabilidade com SHAP**
```python
from src.evaluation.model_interpretation import ModelInterpreter

interpreter = ModelInterpreter()

# Valores SHAP
shap_values = interpreter.calculate_shap_values(
    model=trained_model,
    X_test=X_test,
    model_type="tree"  # ou "linear", "deep"
)

# Visualiza√ß√µes SHAP
interpreter.plot_shap_summary(shap_values, X_test)
interpreter.plot_shap_waterfall(shap_values, X_test, instance_idx=0)
interpreter.plot_shap_dependence(shap_values, X_test, feature="feature1")
```

#### 6. üåê API REST com FastAPI

**Servir Modelos em Produ√ß√£o**
```python
from fastapi import FastAPI, HTTPException
from src.deployment.api import ModelAPI

app = FastAPI(title="ML Model API", version="1.0.0")
model_api = ModelAPI()

@app.post("/predict")
async def predict(request: PredictionRequest):
    try:
        # Carregar modelo
        model = model_api.load_model("production_model_v1.pkl")
        
        # Fazer predi√ß√£o
        prediction = model_api.predict(
            model=model,
            features=request.features
        )
        
        return {
            "prediction": prediction,
            "model_version": "v1.0",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch_predict")
async def batch_predict(request: BatchPredictionRequest):
    # Predi√ß√µes em lote
    predictions = model_api.batch_predict(
        model_name=request.model_name,
        data=request.data
    )
    return {"predictions": predictions}
```

#### 7. üìä MLflow para Tracking

**Tracking de Experimentos**
```python
import mlflow
import mlflow.sklearn
from src.utils.mlflow_utils import MLflowTracker

tracker = MLflowTracker()

# Iniciar experimento
with mlflow.start_run():
    # Log par√¢metros
    mlflow.log_params({
        "n_estimators": 100,
        "max_depth": 10,
        "learning_rate": 0.1
    })
    
    # Treinar modelo
    model = XGBClassifier(**params)
    model.fit(X_train, y_train)
    
    # Avaliar modelo
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    # Log m√©tricas
    mlflow.log_metrics({
        "accuracy": accuracy,
        "precision": precision_score(y_test, y_pred, average="weighted"),
        "recall": recall_score(y_test, y_pred, average="weighted")
    })
    
    # Log modelo
    mlflow.sklearn.log_model(model, "model")
    
    # Log artefatos
    mlflow.log_artifact("feature_importance.png")
```

### üê≥ Containeriza√ß√£o e Deployment

#### Docker para Desenvolvimento
```dockerfile
# Dockerfile.training
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ ./src/
COPY data/ ./data/
COPY configs/ ./configs/

CMD ["python", "scripts/train_model.py"]
```

#### Docker para Produ√ß√£o
```dockerfile
# Dockerfile.api
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ ./src/
COPY models/production/ ./models/

EXPOSE 8000

CMD ["uvicorn", "src.deployment.api:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### Kubernetes Deployment
```yaml
# kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-api
  template:
    metadata:
      labels:
        app: ml-api
    spec:
      containers:
      - name: ml-api
        image: ml-pipeline:latest
        ports:
        - containerPort: 8000
        env:
        - name: MODEL_PATH
          value: "/app/models/production_model.pkl"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
```

### üìä Dashboard Streamlit

**Interface Interativa**
```python
import streamlit as st
from src.streamlit_app.components import ModelDashboard

st.set_page_config(
    page_title="ML Pipeline Dashboard",
    page_icon="ü§ñ",
    layout="wide"
)

dashboard = ModelDashboard()

# Sidebar para sele√ß√£o
st.sidebar.title("ML Pipeline Dashboard")
page = st.sidebar.selectbox(
    "Selecione uma p√°gina",
    ["Vis√£o Geral", "Experimentos", "Modelos", "Predi√ß√µes", "Monitoramento"]
)

if page == "Vis√£o Geral":
    dashboard.show_overview()
elif page == "Experimentos":
    dashboard.show_experiments()
elif page == "Modelos":
    dashboard.show_models()
elif page == "Predi√ß√µes":
    dashboard.show_predictions()
elif page == "Monitoramento":
    dashboard.show_monitoring()
```

### üéØ Compet√™ncias Demonstradas

#### Machine Learning
- ‚úÖ **Algoritmos Supervisionados**: Classifica√ß√£o e regress√£o
- ‚úÖ **Algoritmos N√£o-Supervisionados**: Clustering e redu√ß√£o de dimensionalidade
- ‚úÖ **Ensemble Methods**: Voting, bagging, boosting, stacking
- ‚úÖ **Deep Learning**: Redes neurais com TensorFlow/PyTorch

#### MLOps e DevOps
- ‚úÖ **Versionamento**: Git, DVC para dados e modelos
- ‚úÖ **Containeriza√ß√£o**: Docker, Kubernetes
- ‚úÖ **CI/CD**: GitHub Actions, automated testing
- ‚úÖ **Monitoramento**: MLflow, Wandb, Evidently

#### Engenharia de Software
- ‚úÖ **Arquitetura**: Design patterns, SOLID principles
- ‚úÖ **Testes**: Unit tests, integration tests, pytest
- ‚úÖ **Documenta√ß√£o**: Docstrings, README, API docs
- ‚úÖ **Performance**: Profiling, optimization, caching

### üöÄ Quick Start

#### Instala√ß√£o Local
```bash
# Clonar reposit√≥rio
git clone https://github.com/galafis/python-ml-pipeline-complete.git
cd python-ml-pipeline-complete

# Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Instalar depend√™ncias
pip install -r requirements.txt

# Configurar MLflow
mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns

# Treinar modelo
python scripts/train_model.py --config configs/model_config.yaml

# Iniciar API
uvicorn src.deployment.api:app --reload

# Iniciar dashboard
streamlit run streamlit_app/app.py
```

#### Docker Compose
```bash
# Iniciar todos os servi√ßos
docker-compose up -d

# Servi√ßos dispon√≠veis:
# - API: http://localhost:8000
# - MLflow: http://localhost:5000
# - Streamlit: http://localhost:8501
```

### üìà Casos de Uso Pr√°ticos

#### 1. E-commerce: Recomenda√ß√£o de Produtos
- Algoritmos colaborativos e baseados em conte√∫do
- Features de comportamento do usu√°rio
- A/B testing para otimiza√ß√£o

#### 2. Finan√ßas: Detec√ß√£o de Fraude
- Modelos de anomalia em tempo real
- Features de transa√ß√£o e comportamento
- Alertas autom√°ticos

#### 3. Sa√∫de: Diagn√≥stico Assistido
- Classifica√ß√£o de imagens m√©dicas
- An√°lise de dados cl√≠nicos
- Interpretabilidade para m√©dicos

#### 4. Marketing: Segmenta√ß√£o de Clientes
- Clustering de comportamento
- Predi√ß√£o de churn
- Otimiza√ß√£o de campanhas

---

## üá∫üá∏ English

### üöÄ Overview

**Complete Machine Learning pipeline** developed in Python, implementing MLOps best practices:

- üîÑ **End-to-End Pipeline**: From data ingestion to deployment
- üß† **Multiple Algorithms**: Classification, regression, clustering
- üìä **Monitoring**: Experiment tracking with MLflow
- üê≥ **Containerization**: Docker for consistent deployment
- üåê **REST API**: FastAPI for serving models in production
- üìà **Visualizations**: Interactive dashboards with Streamlit

### üéØ Pipeline Objectives

- **Automate** the entire ML lifecycle
- **Standardize** development processes
- **Facilitate** deployment and monitoring
- **Ensure** experiment reproducibility
- **Accelerate** model time-to-market

### üöÄ Main Features

#### 1. üìä Data Ingestion and Processing
- Flexible data ingestion from multiple sources
- Automated data validation and quality checks
- Feature engineering and selection
- Data preprocessing pipelines

#### 2. üß† Machine Learning Models
- Classification and regression algorithms
- Ensemble methods and model stacking
- Hyperparameter optimization with Optuna
- Cross-validation and model selection

#### 3. üìà Evaluation and Interpretability
- Comprehensive evaluation metrics
- SHAP values for model interpretation
- Feature importance analysis
- Model performance visualization

#### 4. üåê Production Deployment
- FastAPI REST API for model serving
- Docker containerization
- Kubernetes orchestration
- Batch prediction capabilities

#### 5. üìä MLOps and Monitoring
- MLflow for experiment tracking
- Model versioning and registry
- Performance monitoring
- Data drift detection

### üéØ Skills Demonstrated

#### Machine Learning
- ‚úÖ **Supervised Algorithms**: Classification and regression
- ‚úÖ **Unsupervised Algorithms**: Clustering and dimensionality reduction
- ‚úÖ **Ensemble Methods**: Voting, bagging, boosting, stacking
- ‚úÖ **Deep Learning**: Neural networks with TensorFlow/PyTorch

#### MLOps and DevOps
- ‚úÖ **Versioning**: Git, DVC for data and models
- ‚úÖ **Containerization**: Docker, Kubernetes
- ‚úÖ **CI/CD**: GitHub Actions, automated testing
- ‚úÖ **Monitoring**: MLflow, Wandb, Evidently

#### Software Engineering
- ‚úÖ **Architecture**: Design patterns, SOLID principles
- ‚úÖ **Testing**: Unit tests, integration tests, pytest
- ‚úÖ **Documentation**: Docstrings, README, API docs
- ‚úÖ **Performance**: Profiling, optimization, caching

---

## üìÑ Licen√ßa | License

MIT License - veja o arquivo [LICENSE](LICENSE) para detalhes | see [LICENSE](LICENSE) file for details

## üìû Contato | Contact

**GitHub**: [@galafis](https://github.com/galafis)  
**LinkedIn**: [Gabriel Demetrios Lafis](https://linkedin.com/in/galafis)  
**Email**: gabriel.lafis@example.com

---

<div align="center">

**Desenvolvido com ‚ù§Ô∏è para Machine Learning em Produ√ß√£o | Developed with ‚ù§Ô∏è for Production Machine Learning**

[![GitHub](https://img.shields.io/badge/GitHub-galafis-blue?style=flat-square&logo=github)](https://github.com/galafis)
[![Python](https://img.shields.io/badge/Python-3776AB?style=flat-square&logo=python&logoColor=white)](https://www.python.org/)

</div>

