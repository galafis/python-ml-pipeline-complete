# ML Model Configuration Template
# Configuration for different ML models with hyperparameters

models:
  # Random Forest Classifier
  random_forest:
    type: "classifier"
    params:
      n_estimators: 100
      max_depth: 10
      min_samples_split: 2
      min_samples_leaf: 1
      random_state: 42
    hyperparameter_ranges:
      n_estimators: [50, 100, 200, 300]
      max_depth: [5, 10, 15, 20, null]
      min_samples_split: [2, 5, 10]
      min_samples_leaf: [1, 2, 4]

  # XGBoost Classifier
  xgboost:
    type: "classifier"
    params:
      n_estimators: 200
      learning_rate: 0.1
      max_depth: 6
      subsample: 0.8
      colsample_bytree: 0.8
      random_state: 42
    hyperparameter_ranges:
      n_estimators: [100, 200, 300, 500]
      learning_rate: [0.01, 0.05, 0.1, 0.2, 0.3]
      max_depth: [3, 4, 5, 6, 7, 8]
      subsample: [0.6, 0.7, 0.8, 0.9, 1.0]
      colsample_bytree: [0.6, 0.7, 0.8, 0.9, 1.0]

  # LightGBM Classifier
  lightgbm:
    type: "classifier"
    params:
      n_estimators: 200
      learning_rate: 0.1
      max_depth: 6
      num_leaves: 31
      subsample: 0.8
      colsample_bytree: 0.8
      random_state: 42
    hyperparameter_ranges:
      n_estimators: [100, 200, 300, 500]
      learning_rate: [0.01, 0.05, 0.1, 0.2]
      max_depth: [3, 4, 5, 6, 7]
      num_leaves: [15, 31, 63, 127]
      subsample: [0.6, 0.8, 1.0]
      colsample_bytree: [0.6, 0.8, 1.0]

  # Support Vector Machine
  svm:
    type: "classifier"
    params:
      C: 1.0
      kernel: "rbf"
      gamma: "scale"
      random_state: 42
    hyperparameter_ranges:
      C: [0.1, 1.0, 10.0, 100.0]
      kernel: ["linear", "poly", "rbf", "sigmoid"]
      gamma: ["scale", "auto", 0.001, 0.01, 0.1, 1.0]

# Ensemble Methods
ensemble:
  voting_classifier:
    type: "ensemble"
    voting: "soft"
    models: ["random_forest", "xgboost", "lightgbm"]
    
  stacking_classifier:
    type: "ensemble"
    final_estimator: "lightgbm"
    cv: 5
    models: ["random_forest", "xgboost", "svm"]

# Model Training Configuration
training:
  cross_validation:
    cv_folds: 5
    scoring: "accuracy"
    
  test_split:
    test_size: 0.2
    random_state: 42
    stratify: true
    
  validation_split:
    val_size: 0.2
    random_state: 42

# Hyperparameter Tuning Configuration
hyperparameter_tuning:
  method: "optuna"  # optuna, grid_search, random_search
  n_trials: 100
  timeout: 3600  # seconds
  n_jobs: -1
  
  optimization:
    direction: "maximize"
    metric: "accuracy"
    
  pruning:
    enabled: true
    patience: 10

# Model Evaluation Metrics
evaluation_metrics:
  classification:
    - accuracy
    - precision
    - recall
    - f1_score
    - roc_auc
    - confusion_matrix
    - classification_report
    
  regression:
    - mse
    - rmse
    - mae
    - r2_score
    - mean_absolute_percentage_error

# Early Stopping Configuration
early_stopping:
  enabled: true
  patience: 10
  min_delta: 0.001
  monitor: "val_accuracy"
  restore_best_weights: true
